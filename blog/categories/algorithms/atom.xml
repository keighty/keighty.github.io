<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Algorithms | keighty]]></title>
  <link href="http://www.katieleonard.ca/blog/categories/algorithms/atom.xml" rel="self"/>
  <link href="http://www.katieleonard.ca/"/>
  <updated>2018-02-23T09:56:13-08:00</updated>
  <id>http://www.katieleonard.ca/</id>
  <author>
    <name><![CDATA[katie leonard]]></name>
    <email><![CDATA[keighty.leonard@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Least Recently Used (LRU) Cache]]></title>
    <link href="http://www.katieleonard.ca/blog/2018/least-recently-used/"/>
    <updated>2018-02-23T00:00:00-08:00</updated>
    <id>http://www.katieleonard.ca/blog/2018/least-recently-used</id>
    <content type="html"><![CDATA[<p>I was recently challenged to implement a least recently used (LRU) cache in javascript, which taxed both my object-oriented javascripting chops as well as my hazy memory of how to implement a linked list.<!--more--> An LRU cache discards least recently used items first (<a href="https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_Recently_Used_(LRU">wikipedia</a>)). Not only do you have to limit the size of the list, but you have to track when an item has been accessed recently and ensure it is not removed before an item that has been accessed less recently.</p>

<p>Judging from the scores of implementations available from my first Google search, this appears to be a fairly common interview question. Here is the approach that I took:</p>

<ol>
<li>Maintain performance by storing items in a hash map</li>
<li>Use a queue-type linked list to keep track of access order: a new item is added to the end of the queue, while an existing item is removed from its place in the queue and re-added to the end of the queue.</li>
<li>Keep the cache at the specified size by lopping off the head of the list (and removing those items from the map).</li>
<li>Do it all using object oriented design principles.</li>
</ol>


<h3>The API/object design</h3>

<p>A simple cache should not have too many bells and whistles. It needs a <code>get</code> and a <code>set</code>, and possbily a <code>size</code> (useful for testing, probably not something most users desperately need). It should take a specific <code>cacheSize</code> as well, so we can set limits on the cache at startup.</p>

<p>```javascript
const LRU = function (cacheSize) {
  this._maxSize = cacheSize</p>

<p>  this.get = (key) => {}
  this.set = (key, value) => {}
  this.size = () => {}
}
```</p>

<h3>The data structures</h3>

<p>To access stored values in the cache in the most performant way, we store them with a key in a hash map:</p>

<p><code>javascript
// ...
this._map = new Map()
// ...
</code></p>

<p>In order to keep track of order access, we need to keep references to the head and the tail of the list, as well as an object to store the cached value and its place in the queue:</p>

<p>```javascript
//&hellip;
this.<em>head = null
this.</em>tail = null
//&hellip;</p>

<p>const Node = function(key, value) {
  this.key = key;
  this.value = value;
  this.next = null;
  this.prev = null;
};
```</p>

<h3>The implementaton</h3>

<p>```javascript
const LRU = function(cacheSize) {
  this.<em>maxSize = cacheSize;
  this.</em>map = new Map();
  this.<em>head = null;
  this.</em>tail = null;</p>

<p>  this.size = () => this._map.size;</p>

<p>  this.get = key => {</p>

<pre><code>const item = this._map.get(key);

if (item) {
  this._dequeue(item);
  this._enqueue(item);
  return item.value;
}
return;
</code></pre>

<p>  };</p>

<p>  this.set = (key, value) => {</p>

<pre><code>const node = new Node(key, value);

if (this._map.has(key)) {
  const existingNode = this._map.get(key);
  this._dequeue(existingNode);
}

this._enqueue(node);
this._trimList();

return value;
</code></pre>

<p>  };</p>

<p>  this._trimList = () => {</p>

<pre><code>if (this._map.size &gt; this._maxSize) {
  this._dequeue(this._head);
}
</code></pre>

<p>  };</p>

<p>  this._enqueue = node => {</p>

<pre><code>this._map.set(node.key, node);
if (!this._head) {
  this._head = node;
  this._tail = node;
} else {
  this._tail.next = node;
  node.prev = this._tail;
  node.next = null;
  this._tail = node;
}
return node.value;
</code></pre>

<p>  };</p>

<p>  this._dequeue = node => {</p>

<pre><code>if (this._head === node) {
  const { next, key } = this._head;
  if (next) {
    this._head = next;
    this._head.prev = null;
  } else {
    this._head = null;
    this._tail = null;
  }
} else {
  const { prev, next } = node;
  prev.next = next;
  next.prev = prev;
}
this._map.delete(node.key);
</code></pre>

<p>  };</p>

<p>  this.toString = () => {</p>

<pre><code>const itr = this._map.entries();
const entries = [];
for (let item of itr) {
  const [key, node] = item;
  const { value } = node;
  entries.push([key, value]);
}
return entries;
</code></pre>

<p>  };
};</p>

<p>const Node = function(key, value) {
  this.key = key;
  this.value = value;
};</p>

<p>module.exports = LRU;
```</p>

<p>Checkout <a href="https://github.com/keighty/lru-cache">this repo</a> for the full implementation and tests.</p>
]]></content>
  </entry>
  
</feed>
